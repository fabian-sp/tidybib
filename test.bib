% Encoding: UTF-8

@Article{Beck2003,
  author   = {Beck, Amir and Teboulle, Marc},
  journal  = {Oper. Res. Lett.},
  title    = {Mirror descent and nonlinear projected subgradient methods for convex optimization},
  year     = {2003},
  issn     = {0167-6377},
  number   = {3},
  pages    = {167--175},
  volume   = {31},
  fjournal = {Operations Research Letters},
  mrclass  = {90C25},
  mrnumber = {1967286},
  true_doi = {10.1016/S0167-6377(02)00231-6},
  url      = {https://doi-org.eaccess.ub.tum.de/10.1016/S0167-6377(02)00231-6},
}

@Article{Loizou2020,
  author        = {Nicolas Loizou and Sharan Vaswani and Issam Laradji and Simon Lacoste-Julien},
  title         = {Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence},
  year          = {2020},
  month         = feb,
  abstract      = {We propose a stochastic variant of the classical Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although computing the Polyak step-size requires knowledge of the optimal function values, this information is readily available for typical modern machine learning applications. Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive choice for setting the learning rate for stochastic gradient descent (SGD). We provide theoretical convergence guarantees for SGD equipped with SPS in different settings, including strongly convex, convex and non-convex functions. Furthermore, our analysis results in novel convergence guarantees for SGD with a constant step-size. We show that SPS is particularly effective when training over-parameterized models capable of interpolating the training data. In this setting, we prove that SPS enables SGD to converge to the true solution at a fast rate without requiring the knowledge of any problem-dependent constants or additional computational overhead. We experimentally validate our theoretical results via extensive experiments on synthetic and real datasets. We demonstrate the strong performance of SGD with SPS compared to state-of-the-art optimization methods when training over-parameterized models.},
  archiveprefix = {arXiv},
  file          = {:http\://arxiv.org/pdf/2002.10542v3:PDF},
  keywords      = {math.OC, cs.LG, stat.ML},
  primaryclass  = {math.OC},
  url           = {http://arxiv.org/pdf/2002.10542v3},
}

@Article{Beck2020,
  author     = {Beck, Amir and Hallak, Nadav},
  title      = {On the convergence to stationary points of deterministic and randomized feasible descent directions methods},
  doi        = {10.1137/18M1217760},
  issn       = {1052-6234},
  number     = {1},
  pages      = {56--79},
  volume     = {30},
  fjournal   = {SIAM Journal on Optimization},
  journal    = {SIAM J. Optim.},
  mrclass    = {90C26 (90C30 90C46)},
  mrnumber   = {4046783},
  mrreviewer = {Douglas S. Gon\c{c}alves},
  year       = {2020},
}



%%%%%%%%%%%%
% Added by Johannes
%%%%%%%%%%%%

@Article{Mishchenko2021,
  author      = {Konstantin Mishchenko},
  title       = {Regularized {N}ewton Method with Global ${O}(1/k^2)$ Convergence},
  year        = {2021},
  date        = {2021-12-03},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/2112.02089:PDF},
  groups      = {[milz:]},
  keywords    = {math.OC, cs.LG},
  url         = {http://arxiv.org/pdf/2112.02089},
}

@Article{Hendrikx2021,
  author   = {Hendrikx, Hadrien and Bach, Francis and Massouli\'{e}, Laurent},
  title    = {An optimal algorithm for decentralized finite-sum optimization},
  doi      = {10.1137/20M134842X},
  issn     = {1052-6234},
  number   = {4},
  pages    = {2753--2783},
  url      = {https://doi-org.eaccess.ub.tum.de/10.1137/20M134842X},
  volume   = {31},
  fjournal = {SIAM Journal on Optimization},
  journal  = {SIAM J. Optim.},
  mrclass  = {90C25 (65C20 65K05 68T05 90C15)},
  mrnumber = {4332972},
  year     = {2021},
}

@Article{Jiang2022,
  author   = {Jiang, Xin and Vandenberghe, Lieven},
  title    = {Bregman primal-dual first-order method and application to sparse semidefinite programming},
  doi      = {10.1007/s10589-021-00339-7},
  issn     = {0926-6003},
  number   = {1},
  pages    = {127--159},
  url      = {https://doi-org.eaccess.ub.tum.de/10.1007/s10589-021-00339-7},
  volume   = {81},
  fjournal = {Computational Optimization and Applications. An International Journal},
  journal  = {Comput. Optim. Appl.},
  mrclass  = {90C22 (65K05 65K10 90C46)},
  mrnumber = {4359467},
  year     = {2022},
}

@Article{Khamaru2019,
  author      = {Koulik Khamaru and Martin J. Wainwright},
  title       = {Convergence guarantees for a class of non-convex and non-smooth optimization problems},
  year        = {2019},
  abstract    = {We consider the problem of finding critical points of functions that are non-convex and non-smooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simplification of the popular CCCP algorithm, used for optimizing functions that can be written as a difference of two convex functions. Our simplified algorithm retains all the convergence properties of CCCP, along with a significantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.},
  date        = {2018-04-25},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\:/arxiv.org/pdf/1804.09629v1:PDF},
  keywords    = {stat.ML, cs.LG, math.OC},
  url         = {http:/arxiv.org/pdf/1804.09629v1},
}

@Book{Lan2020,
  author    = {Lan, Guanghui},
  title     = {First-order and {S}tochastic {O}ptimization {M}ethods for {M}achine {L}earning},
  doi       = {10.1007/978-3-030-39568-1},
  publisher = {Springer},
  series    = {Springer Ser. Data Sci.},
  address   = {Cham},
  timestamp = {2020.08.03},
  year      = {2020},
}

@Article{Wang2018,
  author    = {Yu Wang and Wotao Yin and Jinshan Zeng},
  title     = {Global Convergence of {ADMM} in Nonconvex Nonsmooth Optimization},
  doi       = {10.1007/s10915-018-0757-z},
  number    = {1},
  pages     = {29--63},
  url       = {https://doi.org/10.1007/s10915-018-0757-z},
  volume    = {78},
  journal   = {J. Sci. Comput.},
  month     = jun,
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2018},
}

@Article{VanBarel2021,
  author   = {Van Barel, Andreas and Vandewalle, Stefan},
  title    = {M{G}/{OPT} and multilevel {M}onte {C}arlo for robust optimization of {PDE}s},
  doi      = {10.1137/20M1347164},
  issn     = {1052-6234},
  number   = {3},
  pages    = {1850--1876},
  url      = {https://doi-org.eaccess.ub.tum.de/10.1137/20M1347164},
  volume   = {31},
  fjournal = {SIAM Journal on Optimization},
  journal  = {SIAM J. Optim.},
  mrclass  = {49M41 (35Q93 49J55 49M05 65C05 65K10)},
  mrnumber = {4287357},
  year     = {2021},
}

@Article{Boob2022,
  author  = {Boob, D. and Deng, Q. and Lan, G},
  title   = {Stochastic first-order methods for convex and nonconvex functional constrained optimization},
  doi     = {10.1007/s10107-021-01742-y},
  journal = {Math. Program},
  year    = {2022},
}

@Article{Kanzow2021,
  author     = {Kanzow, Christian and Lechner, Theresa},
  title      = {Globalized inexact proximal {N}ewton-type methods for nonconvex composite functions},
  doi        = {10.1007/s10589-020-00243-6},
  issn       = {0926-6003},
  number     = {2},
  pages      = {377--410},
  volume     = {78},
  fjournal   = {Computational Optimization and Applications. An International Journal},
  journal    = {Comput. Optim. Appl.},
  mrclass    = {90C26 (65K10 90C53)},
  mrnumber   = {4215215},
  mrreviewer = {Vladimir Shikhman},
  year       = {2021},
}

@Article{Sabach2022,
  author   = {Sabach, Shoham and Teboulle, Marc},
  title    = {Faster {L}agrangian-based methods in convex optimization},
  doi      = {10.1137/20M1375358},
  issn     = {1052-6234},
  number   = {1},
  pages    = {204--227},
  url      = {https://doi-org.eaccess.ub.tum.de/10.1137/20M1375358},
  volume   = {32},
  fjournal = {SIAM Journal on Optimization},
  journal  = {SIAM J. Optim.},
  mrclass  = {90C25 (65K05 65K10)},
  mrnumber = {4386487},
  year     = {2022},
}

@Article{Drusvyatskiy2020,
  author        = {Dmitriy Drusvyatskiy and Lin Xiao},
  title         = {Stochastic optimization with decision-dependent distributions},
  year          = {2020},
  month         = nov,
  abstract      = {Stochastic optimization problems often involve data distributions that change in reaction to the decision variables. This is the case for example when members of the population respond to a deployed classifier by manipulating their features so as to improve the likelihood of being positively labeled. Recent works on performative prediction have identified an intriguing solution concept for such problems: find the decision that is optimal with respect to the static distribution that the decision induces. Continuing this line of work, we show that typical stochastic algorithms -- originally designed for static problems -- can be applied directly for finding such equilibria with little loss in efficiency. The reason is simple to explain: the main consequence of the distributional shift is that it corrupts algorithms with a bias that decays linearly with the distance to the solution. Using this perspective, we obtain sharp convergence guarantees for popular algorithms, such as stochastic gradient, clipped gradient, proximal point, and dual averaging methods, along with their accelerated and proximal variants. In realistic applications, deployment of a decision rule is often much more expensive than sampling. We show how to modify the aforementioned algorithms so as to maintain their sample efficiency while performing only logarithmically many deployments.},
  archiveprefix = {arXiv},
  file          = {:http\://arxiv.org/pdf/2011.11173v2:PDF},
  keywords      = {math.OC, 90C15, 90C25},
  primaryclass  = {math.OC},
  url           = {https://arxiv.org/pdf/2011.11173.pdf},
}

@Article{Quirynen2018,
  author  = {Quirynen, R. and Gros, S. and Diehl, M.},
  journal = {SIAM J. Optim.},
  title   = {Inexact {N}ewton-{T}ype {O}ptimization with {I}terated {S}ensitivities},
  year    = {2018},
  number  = {1},
  pages   = {74--95},
  volume  = {28},
}

@Article{VanAckooij2018,
  author  = {Van Ackooij, W. and Frangioni, A.},
  journal = {SIAM J. Optim.},
  title   = {Incremental bundle methods using upper models},
  year    = {2018},
}

@Article{Qin2013,
  author    = {Qin, Z. and Scheinberg, K. and Goldfarb, D.},
  journal   = {Math. Program. Comput.},
  title     = {Efficient block-coordinate descent algorithms for the group lasso},
  year      = {2013},
  number    = {2},
  pages     = {143--169},
  volume    = {5},
  publisher = {Springer},
}

@InCollection{Monteiro2003,
  author     = {Monteiro, Renato D. C.},
  title      = {First- and second-order methods for semidefinite programming},
  year       = {2003},
  note       = {ISMP, 2003 (Copenhagen)},
  number     = {1-2, Ser. B},
  pages      = {209--244},
  volume     = {97},
  doi        = {10.1007/s10107-003-0451-1},
  fjournal   = {Mathematical Programming. A Publication of the Mathematical Programming Society},
  issn       = {0025-5610},
  journal    = {Math. Program.},
  mrclass    = {90C22},
  mrnumber   = {2004397},
  mrreviewer = {Franz Rendl},
  url        = {https://doi-org.eaccess.ub.tum.de/10.1007/s10107-003-0451-1},
}

@Article{Qian2021,
  author        = {Qian LI and Binyan Jiang and Defeng Sun},
  title         = {MARS: A second-order reduction algorithm for high-dimensional sparse precision matrices estimation},
  year          = {2021},
  month         = jun,
  abstract      = {Estimation of the precision matrix (or inverse covariance matrix) is of great importance in statistical data analysis. However, as the number of parameters scales quadratically with the dimension p, computation becomes very challenging when p is large. In this paper, we propose an adaptive sieving reduction algorithm to generate a solution path for the estimation of precision matrices under the $\ell_1$ penalized D-trace loss, with each subproblem being solved by a second-order algorithm. In each iteration of our algorithm, we are able to greatly reduce the number of variables in the problem based on the Karush-Kuhn-Tucker (KKT) conditions and the sparse structure of the estimated precision matrix in the previous iteration. As a result, our algorithm is capable of handling datasets with very high dimensions that may go beyond the capacity of the existing methods. Moreover, for the sub-problem in each iteration, other than solving the primal problem directly, we develop a semismooth Newton augmented Lagrangian algorithm with global linear convergence on the dual problem to improve the efficiency. Theoretical properties of our proposed algorithm have been established. In particular, we show that the convergence rate of our algorithm is asymptotically superlinear. The high efficiency and promising performance of our algorithm are illustrated via extensive simulation studies and real data applications, with comparison to several state-of-the-art solvers.},
  archiveprefix = {arXiv},
  eprint        = {2106.13508},
  file          = {:http\://arxiv.org/pdf/2106.13508v1:PDF},
  keywords      = {stat.CO, math.OC, stat.ME},
  primaryclass  = {stat.CO},
}

@Comment{jabref-meta: databaseType:bibtex;}
